{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import openai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the API key from environment variables\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    nombre, extension = os.path.splitext(file) \n",
    "    if extension == '.html':\n",
    "        from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "        print(f'load {file}...')\n",
    "        loader = UnstructuredHTMLLoader(file)\n",
    "    elif extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader  \n",
    "        print(f'load {file}...')\n",
    "        loader = TextLoader(file)\n",
    "    elif extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'load {file}...')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'load {file}...')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('The document format is not supported!')\n",
    "        return None\n",
    "\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../test.pdf...\n",
      "[Document(page_content=\"Title: Whiskers' Midnight Adventure  \\nIn the quiet town of Meadowville, under the glow of a silver moon, there lived a curious cat \\nnamed Whiskers. With fur as black as the night and eyes that shimmered like stars, \\nWhiskers was known for his adventurous spirit.  \\nOne night, while his human family slept soundly, Whiskers heard a peculiar sound coming \\nfrom the kitchen. His ears perked up, and his paws silently carried him towards the source. \\nThe moonlight streamed through the window, casting shadows that danced on th e walls as \\nWhiskers crept closer.  \\nAs he peered around the corner, he saw a small mouse, its fur glistening under the moon’s \\nlight. The mouse, seemingly unaware of the cat’s presence, continued nibbling on a piece \\nof cheese it had found. Whiskers, with a flick of his tail, prepared to pounc e. \\nBut just as he leaped, the mouse scurried away with astonishing speed, disappearing \\nunder the refrigerator. Whiskers, puzzled but intrigued, decided to wait. Minutes turned into \\nhours, and still, the mouse did not reappear. As the first light of dawn crept  into the sky, \\nWhiskers realized it was time to retreat.  \\nWith a final glance at the spot where the mouse had vanished, Whiskers trotted back to his \\ncozy bed by the fireplace. He curled up, thinking about the night’s adventure, and as his \\neyes closed, he dreamed of the brave little mouse that got away.  \\nAnd so, Whiskers’ midnight adventure ended not with a feast, but with a new respect for the \\nclever creature that shared his home. From that night on, whenever Whiskers went on his \\nnightly prowls, he always kept an eye out for his little friend, hoping perh aps to meet again \\nunder the moonlit sky.  \\n \", metadata={'source': '../test.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "document = \"../test.pdf\"\n",
    "content = load_document(document)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_chunk_size(document_length, complexity_rating=None):\n",
    "    # Define thresholds for document length\n",
    "    if document_length < 5000:  # example threshold for characters\n",
    "        return 2000  # Larger chunks for shorter documents\n",
    "    elif document_length < 20000:\n",
    "        return 1500  # Moderate chunk size\n",
    "    else:\n",
    "        return 1000  # Smaller chunks for very long documents\n",
    "\n",
    "def split_document(document, complexity_rating=None):\n",
    "    document_length = len(document)  # Measure document length\n",
    "    chunk_size = dynamic_chunk_size(document_length, complexity_rating)\n",
    "    \n",
    "    # Your existing splitting logic, adjusted for dynamic chunk size\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)\n",
    "    fragments = text_splitter.split_documents(document)\n",
    "    return fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = split_document(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Title: Whiskers' Midnight Adventure  \\nIn the quiet town of Meadowville, under the glow of a silver moon, there lived a curious cat \\nnamed Whiskers. With fur as black as the night and eyes that shimmered like stars, \\nWhiskers was known for his adventurous spirit.  \\nOne night, while his human family slept soundly, Whiskers heard a peculiar sound coming \\nfrom the kitchen. His ears perked up, and his paws silently carried him towards the source. \\nThe moonlight streamed through the window, casting shadows that danced on th e walls as \\nWhiskers crept closer.  \\nAs he peered around the corner, he saw a small mouse, its fur glistening under the moon’s \\nlight. The mouse, seemingly unaware of the cat’s presence, continued nibbling on a piece \\nof cheese it had found. Whiskers, with a flick of his tail, prepared to pounc e. \\nBut just as he leaped, the mouse scurried away with astonishing speed, disappearing \\nunder the refrigerator. Whiskers, puzzled but intrigued, decided to wait. Minutes turned into \\nhours, and still, the mouse did not reappear. As the first light of dawn crept  into the sky, \\nWhiskers realized it was time to retreat.  \\nWith a final glance at the spot where the mouse had vanished, Whiskers trotted back to his \\ncozy bed by the fireplace. He curled up, thinking about the night’s adventure, and as his \\neyes closed, he dreamed of the brave little mouse that got away.  \\nAnd so, Whiskers’ midnight adventure ended not with a feast, but with a new respect for the \\nclever creature that shared his home. From that night on, whenever Whiskers went on his \\nnightly prowls, he always kept an eye out for his little friend, hoping perh aps to meet again \\nunder the moonlit sky.\", metadata={'source': '../test.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split (data, chunk_size=1500):\n",
    "#     from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=20)\n",
    "#     fragments = text_splitter.split_documents(data)\n",
    "#     return fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fragments = split(content)\n",
    "# print(len(fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `fragments` is a list of Document objects\n",
    "text_fragments = [doc.page_content for doc in fragments]\n",
    "print(len(text_fragments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embed(text):\n",
    "    batch_result = embeddings.embed_query(text)\n",
    "    return batch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "metadata = []\n",
    "\n",
    "for item in text_fragments:\n",
    "    # Assuming `batch_embed` function returns a vector for the item\n",
    "    vector = batch_embed(item)\n",
    "    vectors.append(vector)\n",
    "    # Create metadata for each item; here, we just store the item text itself\n",
    "    metadata.append({\"content\": item})\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f\"id_{i}\" for i in range(len(vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ids, vectors, and metadata into the format Pinecone expects\n",
    "data = [{\"id\": id, \"values\": vector, \"metadata\": meta} for id, vector, meta in zip(ids, vectors, metadata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"langchain-test\")\n",
    "\n",
    "index.upsert(\n",
    "  vectors=data,\n",
    "  namespace=\"ns1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How is Whisker's described as?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vector = embeddings.embed_query(user_question)\n",
    "print(user_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index = pc.Index(\"langchain-test\")\n",
    "\n",
    "response = index.query(\n",
    "    namespace=\"ns1\",\n",
    "    vector=user_vector,\n",
    "    top_k=5,\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the matches and their documents from the query response\n",
    "matches = response['matches']\n",
    "documents = [match['metadata']['content'] for match in matches]  # Adjust according to your metadata structure\n",
    "\n",
    "# Initialize an empty string to store the formatted document descriptions\n",
    "formatted_documents = \"\"\n",
    "\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    formatted_documents += f\"Chunk Reference {i}: {doc}\\n\"  # Adding a newline for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Prompt Generation\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"formatted_documents\", \"user_question\"],\n",
    "    template = '''Based on this reference below: \n",
    "    \n",
    "{formatted_documents}\n",
    "\n",
    "Answer the user question: {user_question}\n",
    "    '''\n",
    ")\n",
    "\n",
    "print(prompt.format(formatted_documents = formatted_documents, user_question = user_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API\n",
    "\n",
    "chatopenai = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
    "llmchain_chat = LLMChain(llm = chatopenai, prompt = prompt)\n",
    "llmchain_chat.run({\"formatted_documents\": formatted_documents, \"user_question\": user_question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embedding_request(strings, model=\"text-embedding-ada-002\", max_tokens=4096):\n",
    "    try:\n",
    "        # Create batch request for embeddings\n",
    "        response = openai.embeddings.create(\n",
    "            model=model,\n",
    "            input=strings,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        # Extract embeddings\n",
    "        embeddings = [embedding['embedding'] for embedding in response['data']]\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_fragments)\n",
    "test_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and store them in a list\n",
    "embeddings_list = batch_embedding_request(text_fragments)\n",
    "\n",
    "# Convert list of embeddings to a NumPy array for further manipulation\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Print embeddings\n",
    "print(\"Embeddings Array:\")\n",
    "print(embeddings_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
